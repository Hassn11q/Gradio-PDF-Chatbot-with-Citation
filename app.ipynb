{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from dspy.retrieve.qdrant_rm import QdrantRM\n",
    "import dspy\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from qdrant_client import QdrantClient\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings\n",
    "embeddings = CohereEmbeddings(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"], model=\"embed-multilingual-light-v3.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048, chunk_overlap=128, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Semantic Chunker\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings, breakpoint_threshold_type=\"interquartile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = []\n",
    "\n",
    "loaded_documents = UnstructuredMarkdownLoader(\n",
    "    \"/Users/hassn-/Desktop/dspy-chatbot-with-citation/data/saudi_vision2030_ar.pdf.md\"\n",
    ").load()\n",
    "\n",
    "document_text = \"\\n\".join([doc.page_content for doc in loaded_documents])\n",
    "\n",
    "documents.extend(\n",
    "    text_splitter.split_documents(semantic_splitter.create_documents([document_text]))\n",
    ")\n",
    "\n",
    "chunks = [doc.page_content for doc in documents]\n",
    "\n",
    "doc_id = list(range(1, len(documents) + 1))\n",
    "\n",
    "vectors = embeddings.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qdrant client\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Qdrant collection\n",
    "client.delete_collection(collection_name=\"data\")\n",
    "client.create_collection(\n",
    "    collection_name=\"data\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Upload data to Qdrant\n",
    "client.upload_collection(collection_name=\"data\", ids=doc_id, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Retriever\n",
    "retriever_model = QdrantRM(qdrant_collection_name=\"data\", qdrant_client=client, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "lm = dspy.Cohere(model=\"command-r-plus\", api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dspy module\n",
    "dspy.settings.configure(lm=lm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def select_relevant_chunks_bm25(query, chunks, top_n):\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    tokenized_chunks = [word_tokenize(chunk.lower()) for chunk in chunks]\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    return [chunks[i] for i in top_n_indices]\n",
    "\n",
    "\n",
    "def select_relevant_chunks_cosine(query, chunks, vectors, top_n):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    cosine_similarities = cosine_similarity([query_embedding], vectors)[0]\n",
    "    top_n_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n",
    "    return [chunks[i] for i in top_n_indices]\n",
    "\n",
    "\n",
    "def merge_top_chunks(bm25_chunks, cosine_chunks):\n",
    "    return list(dict.fromkeys(bm25_chunks + cosine_chunks))\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return \" \".join(re.sub(r\"[^\\w\\s]\", \"\", text.lower()).split())\n",
    "\n",
    "\n",
    "def verify_citations(response_dict, top_chunks):\n",
    "    citations = response_dict.get(\"citations\", [])\n",
    "    if not citations:\n",
    "        print(\"Error: No citations found in response\")\n",
    "        return False, []\n",
    "    matches = []\n",
    "    normalized_chunks = [normalize_text(chunk) for chunk in top_chunks]\n",
    "\n",
    "    for citation in citations:\n",
    "        cited_text = citation.get(\"snippet\", \"\")\n",
    "        if not cited_text:\n",
    "            print(f\"Error: Missing snippet in citation: {citation}\")\n",
    "            return False, citations\n",
    "        normalized_citation = normalize_text(cited_text)\n",
    "\n",
    "        match_found = False\n",
    "        for chunk in normalized_chunks:\n",
    "            if (\n",
    "                normalized_citation in chunk\n",
    "                or fuzz.partial_ratio(normalized_citation, chunk) > 90\n",
    "            ):\n",
    "                match_found = True\n",
    "                break\n",
    "        matches.append(match_found)\n",
    "    return all(matches), citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dspy signature\n",
    "class Generate_answer(dspy.Signature):\n",
    "    \"\"\"Answer the questions with citations\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"system prompt with context and instructions\")\n",
    "    question = dspy.InputField(desc=\"question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"JSON formatted answer with citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dspy module\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(Generate_answer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        bm25_chunks = select_relevant_chunks_bm25(question, chunks, 2)\n",
    "        cosine_chunks = select_relevant_chunks_cosine(question, chunks, vectors, 2)\n",
    "        top_chunks = merge_top_chunks(bm25_chunks, cosine_chunks)\n",
    "\n",
    "        context = \"\\n\".join(top_chunks)\n",
    "        system_prompt = f\"\"\"You are a research assistant. Use the provided document snippets to\n",
    "        answer the query. Format your response with citations in structured JSON format:\n",
    "        <response format>\n",
    "        {{\n",
    "        \"response\":\"Your response here.\",\n",
    "        \"citations\":[\n",
    "            {{\n",
    "                \"title\":\"Document Title\",\n",
    "                \"snippet\":\"Exact snippet from the document\"\n",
    "            }}]\n",
    "        }}\n",
    "        </response format>\n",
    "\n",
    "        IMPORTANT CITATION RULES:\n",
    "        1. Each citation MUST be a complete sentence or phrase from the original text. \n",
    "        2. Citations MUST be VERBATIM and EXACT quotes from the provided documents. \n",
    "        3. DO NOT use ellipses (...) or any other shortening techniques in citations.\n",
    "        4. DO NOT paraphrase or modify the original text in any way for citations.\n",
    "        5. If you need to use multiple sentences in citations, include them in full. \n",
    "        6. USE MULTIPLE citations when necessary to fully support your response. \n",
    "        7. Ensure that each citation DIRECTLY supports a specific part of your response. \n",
    "        8. If you cannot find relevant information in the provided documents, state this clearly in your response. \n",
    "\n",
    "        Here are the relevant documents for your query:\n",
    "        {context}\n",
    "\n",
    "        Remember:\n",
    "        1. Citations must be EXACT, COMPLETE sentences or phrases from the provided text.\n",
    "        2. Do not modify, shorten, or paraphrase the original text in your citations.\n",
    "        3. Use multiple citations when necessary to fully support your response.\n",
    "        4. Ensure each citation is directly relevant to the part of your response that it supports.\n",
    "        5. If you cannot find relevant information in the provided documents, clearly state this in your response.\n",
    "\n",
    "        Now, please answer the given query using the provided information and following these guidelines.\n",
    "        \"\"\"\n",
    "\n",
    "        prediction = self.generate_answer(context=system_prompt, question=question)\n",
    "\n",
    "        # Log the raw response for debugging\n",
    "        logging.info(f\"Raw LLM response: {prediction.answer}\")\n",
    "\n",
    "        # Extract the answer and citations from the incomplete response\n",
    "        extracted_answer, extracted_citations = self.extract_answer_and_citations(\n",
    "            prediction.answer\n",
    "        )\n",
    "\n",
    "        if extracted_answer:\n",
    "            citation_check, verified_citations = self.verify_citations(\n",
    "                extracted_citations, top_chunks\n",
    "            )\n",
    "            if citation_check:\n",
    "                return dspy.Prediction(\n",
    "                    context=context,\n",
    "                    answer=extracted_answer,\n",
    "                    citations=verified_citations,\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(\"Citations could not be verified.\")\n",
    "                return dspy.Prediction(\n",
    "                    context=context,\n",
    "                    answer=extracted_answer,\n",
    "                    citations=verified_citations,\n",
    "                    warning=\"Citations could not be verified.\",\n",
    "                )\n",
    "        else:\n",
    "            logging.error(\"Failed to extract answer from LLM response.\")\n",
    "            return dspy.Prediction(\n",
    "                context=context,\n",
    "                answer=\"Failed to generate a valid response.\",\n",
    "                citations=[],\n",
    "                error=\"Invalid response from LLM.\",\n",
    "            )\n",
    "\n",
    "    def extract_answer_and_citations(self, text):\n",
    "        # Extract everything after '\"response\": \"' as the answer\n",
    "        answer_match = re.search(r'\"response\"\\s*:\\s*\"(.+)', text, re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1).strip().rstrip('\",')\n",
    "        else:\n",
    "            answer = None\n",
    "\n",
    "        # Attempt to extract citations, but this might fail if the response is cut off\n",
    "        citations = []\n",
    "        citation_matches = re.findall(r'\"snippet\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        for match in citation_matches:\n",
    "            citations.append({\"title\": \"Document\", \"snippet\": match})\n",
    "\n",
    "        return answer, citations\n",
    "\n",
    "    def verify_citations(self, citations, chunks):\n",
    "        verified_citations = []\n",
    "        for citation in citations:\n",
    "            snippet = citation[\"snippet\"]\n",
    "            if any(snippet in chunk for chunk in chunks):\n",
    "                verified_citations.append(citation)\n",
    "            else:\n",
    "                logging.warning(f\"Citation not found in chunks: {snippet}\")\n",
    "        return len(verified_citations) == len(citations), verified_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG \n",
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chatbot interface\n",
    "def chatbot_interface(user_input, history):\n",
    "    response = rag(user_input)\n",
    "    return response.answer\n",
    "\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chatbot_interface,\n",
    "    title=\"DSPY Chatbot\",\n",
    "    description=\"Ask me anything about Saudi arabia vision 2023\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
